{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "TensorFlow is basically a gradient descent optimization tool and deep learning is just a special case of optimization. As such, let's try out some simple optimization problems and in the process gain more familiarity with the low-level tools. Try to implement the following using gradient descent:\n",
    "\n",
    "## Exercise 1: make one value as close as possible to another\n",
    "Specify a target value (say, 2), and a variable x (initialized to, say, 18) and minimize the distance between x and the target. This task is as simple as it sounds.\n",
    "\n",
    "## Exercise 2: find the mean of vector of numbers\n",
    "The mean of a vector of numbers is value that minimizes the sum of the square distances to all those values. It just so happens that there's an analytic solution to this optimization problem, so it's never solved using optimization. However, just for practice, let's find it using gradient descent!\n",
    "\n",
    "## Exercise 3: invert a square matrix\n",
    "Create a random nxn matrix and use TensorFlow to find the inverse. Construct a scalar loss function, and then minimize it. Then, compare it with the result of directly computing the inverse (e.g., with `np.linalg.inv`). \n",
    "\n",
    "## Exercise 4: find the eigendecomposition of an nxn matrix\n",
    "This is also an introduction to using complex numbers in tensorflow (`tf.complex64`), an uncommon use case, but will be useful later when we model optical fields, which are complex-valued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: make one value as close as possible to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.constant(2, dtype=tf.float32)\n",
    "x = tf.Variable(18, dtype=tf.float32)\n",
    "loss = (x-target)**2 \n",
    "# loss = tf.abs(x-target)\n",
    "\n",
    "# you can reuse this for each exercise (but play around with the learning_rate):\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=.1).minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train_op)\n",
    "    print(sess.run(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: find the mean of vector of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.random.randn(1000)+10  # mean is 10\n",
    "mean = tf.Variable(0, dtype=np.float32)\n",
    "loss = tf.reduce_sum((mean-vec)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(.0001).minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    loss_i, mean_i, _ = sess.run([loss, mean, train_op])\n",
    "    print(mean_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: invert a square matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "matrix = np.random.randn(n, n).astype(np.float32)\n",
    "inverse = tf.get_variable(name='inverse',shape=(n, n), dtype=tf.float32, initializer=tf.random_normal_initializer)\n",
    "identity_pred = tf.matmul(matrix, inverse)\n",
    "loss = tf.reduce_mean((identity_pred-np.eye(5))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(1.).minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "for i in range(10000):\n",
    "    loss_i, pred_i, _ = sess.run([loss, identity_pred, train_op])\n",
    "    losses.append(loss_i)\n",
    "    if i %1000 == 0:\n",
    "        print(loss_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare:\n",
    "print(inverse.eval())\n",
    "print(np.linalg.inv(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: find the eigendecomposition of an nxn matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "A = np.random.randn(n, n).astype(np.float32)\n",
    "\n",
    "Q_re = tf.get_variable(name='Q_real',shape=(n, n), dtype=tf.float32, initializer=tf.random_normal_initializer)\n",
    "Q_im = tf.get_variable(name='Q_imag',shape=(n, n), dtype=tf.float32, initializer=tf.random_normal_initializer)\n",
    "Q = tf.complex(Q_re, Q_im)\n",
    "\n",
    "eigs_re = tf.get_variable(name='eigenvalue_real', shape=(n), initializer=tf.ones_initializer)\n",
    "eigs_im = tf.get_variable(name='eigenvalue_imag', shape=(n), initializer=tf.ones_initializer)\n",
    "eigs = tf.complex(eigs_re, eigs_im)\n",
    "S = tf.linalg.tensor_diag(eigs)\n",
    "\n",
    "factorization = Q @ S @ tf.linalg.inv(Q) # prediction for A\n",
    "loss = tf.reduce_mean(tf.abs(A-factorization)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(.1).minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "# if you get a noninvertible error, try decreasing your learning rate\n",
    "for i in range(20000):\n",
    "    loss_i, _ = sess.run([loss, train_op])\n",
    "    losses.append(loss_i)\n",
    "    if i %1000 == 0:\n",
    "        print(loss_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the optimized eigenvalues?\n",
    "# I normalize them by their magnitude to make it easier to compare to direct calculation\n",
    "# alternatively, we can enforce normalization in the tf graph\n",
    "eigs_pred = eigs.eval()\n",
    "eigs_pred /= np.abs(eigs_pred)\n",
    "print(np.sort(eigs_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to direct calculation:\n",
    "eigs_truth, _ = np.linalg.eig(A)\n",
    "eigs_truth /= np.abs(eigs_truth)\n",
    "print(np.sort(eigs_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
